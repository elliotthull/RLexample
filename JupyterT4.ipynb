{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter server crashed. Unable to connect. \n",
      "\u001b[1;31mError code from Jupyter: 1\n",
      "\u001b[1;31musage: jupyter.py [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "\u001b[1;31m                  [--paths] [--json] [--debug]\n",
      "\u001b[1;31m                  [subcommand]\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mJupyter: Interactive Computing\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mpositional arguments:\n",
      "\u001b[1;31m  subcommand     the subcommand to launch\n",
      "\u001b[1;31m\n",
      "\u001b[1;31moptions:\n",
      "\u001b[1;31m  -h, --help     show this help message and exit\n",
      "\u001b[1;31m  --version      show the versions of core jupyter packages and exit\n",
      "\u001b[1;31m  --config-dir   show Jupyter config dir\n",
      "\u001b[1;31m  --data-dir     show Jupyter data dir\n",
      "\u001b[1;31m  --runtime-dir  show Jupyter runtime dir\n",
      "\u001b[1;31m  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "\u001b[1;31m                 format.\n",
      "\u001b[1;31m  --json         output paths as machine-readable json\n",
      "\u001b[1;31m  --debug        output debug information about paths\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mAvailable subcommands:\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mJupyter command `jupyter-notebook` not found. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "max = 12.0\n",
    "step = 0.05\n",
    "\n",
    "states = np.arange(0, max, step)\n",
    "actions = np.arange(0, max, step)\n",
    "\n",
    "size = round(max / step)\n",
    "\n",
    "T = 4\n",
    "Q = np.zeros([T, size, size])\n",
    "epsilon = 0.15\n",
    "om_q = 0.55\n",
    "om_mu = 0.85\n",
    "gamma = 0.2\n",
    "rho = 0.95\n",
    "softmax_param = 2\n",
    "C = 3\n",
    "mu = np.ones(T)\n",
    "\n",
    "def eps_decay(t):\n",
    "    return epsilon * np.e**(-t)\n",
    "\n",
    "# idk about this factor (gamma in the pseudocode), a value was not mentioned in their description\n",
    "discount = 0.95\n",
    "\n",
    "# given white noise process with specific supports and probabilities\n",
    "supp_W = [0.9, 1.3]\n",
    "pmf_W = [0.75, 0.25]\n",
    "\n",
    "# calculate expectation of white noise process \n",
    "exp_W_gamma = 0\n",
    "for i in range(len(supp_W)):\n",
    "    exp_W_gamma += np.pow(supp_W[i], gamma) * pmf_W[i]\n",
    "\n",
    "# calculate rho * expectation [W ^ gamma]\n",
    "pEWgamma = rho * exp_W_gamma\n",
    "\n",
    "# white noise with two outcomes with probabilities established above\n",
    "def W ():\n",
    "    return np.random.choice(supp_W, p = pmf_W)\n",
    "\n",
    "# given state/action (by index) and mean field investment\n",
    "# return through G(mu, W()) * a (amount invested)\n",
    "# the new state (rounded)\n",
    "# and using formula for utility calculate reward\n",
    "def env (state, action, mu):\n",
    "    consump = states[state] - actions[action]\n",
    "    wealth = actions[action] * W() * C / (pEWgamma * (1 + (C - 1) * np.pow(mu, 3)))\n",
    "    newState = round(wealth / step)\n",
    "    utility = np.pow(consump, gamma) / gamma\n",
    "    return { 'x': newState, 'u': utility }\n",
    "\n",
    "# rho calculator, given we are on the kth episode and\n",
    "# have visited the specific state/action/time pair count_txa times\n",
    "\n",
    "def rhosCalc(count_txa, k):\n",
    "    '''does this have a counter incrementing?'''\n",
    "    rhoQ = 1 / np.pow(1 + count_txa, om_q)\n",
    "    rhoMu = 1 / np.pow(2 + k, om_mu)\n",
    "    return { 'q': rhoQ, 'mu': rhoMu }\n",
    "\n",
    "# eps-greedy policy, takes in 1d array of Q matrix specified by state and time, and state (index)\n",
    "# if Unif[0, 1] > epsilon, choose argmax on 1d array of Q matrix, limited by state\n",
    "# if Unif[0, 1] < epsilon, choose random action, limited by state (unif distribution)\n",
    "def epsAction (Q_x, state, t):\n",
    "    if np.random.random() > eps_decay(t):\n",
    "        maxim = np.max(Q_x[:state + 1])\n",
    "        ind = []\n",
    "        for i in range(0, state + 1):\n",
    "            if maxim == Q_x[i]:\n",
    "                ind.append(i)\n",
    "        return ind[np.random.randint(0, len(ind))]\n",
    "    else:\n",
    "        return np.random.choice(list(range(state + 1)))\n",
    "\n",
    "# utilizing the soft max distribution\n",
    "# calculate the expectation of an action choice\n",
    "def softMaxMean (Q_tx, x_idx):\n",
    "    sum = 0\n",
    "    expectation = 0\n",
    "    weights = np.zeros(x_idx + 1)\n",
    "    for a in range(x_idx + 1):\n",
    "        weights[a] = np.exp(softmax_param * Q_tx[a])\n",
    "        sum += weights[a]\n",
    "        expectation += weights[a] * actions[a]\n",
    "    return expectation / sum\n",
    "\n",
    "num_episodes = 1000000\n",
    "jump = round(num_episodes / 100)\n",
    "\n",
    "mu_k = []\n",
    "for t in range(T):\n",
    "    mu_k.append([])\n",
    "\n",
    "# initialize count for finding rho_Q (learning rate)\n",
    "count_txa = np.zeros([T, len(states), len(actions)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(num_episodes):\n",
    "\n",
    "    # Sample initial state \n",
    "    x_idx = np.random.choice(list(range(0, round(1 / step) + 1)))\n",
    "    \n",
    "    # Episode loop over time periods\n",
    "    for t in range(T):\n",
    "        \n",
    "        # Ensure valid state\n",
    "        if x_idx >= len(states) or x_idx < 0:\n",
    "            break\n",
    "            \n",
    "        # Current Q values at t\n",
    "        Q_xt = Q[t, x_idx, :]\n",
    "        \n",
    "        # Select action\n",
    "        a_idx = epsAction(Q_xt, x_idx, t)\n",
    "        \n",
    "        # Skip if action exceeds state \n",
    "        if a_idx > x_idx:\n",
    "            print(\"act > stat\")\n",
    "            a_idx = x_idx  #Invest everything instead of breaking\n",
    "        \n",
    "        \n",
    "        #call to environment\n",
    "        result = env(x_idx, a_idx, mu[t])\n",
    "        next_x_idx = result['x']\n",
    "        reward = result['u']\n",
    "        \n",
    "        # Check next state is within the state space\n",
    "        next_x_idx = np.min([np.max([next_x_idx, 0]), len(states) - 1])\n",
    "        \n",
    "        # Calculate target\n",
    "        if t < T - 1:\n",
    "            if next_x_idx > 0:\n",
    "                max_next_Q = discount * np.max(Q[t + 1, next_x_idx, :next_x_idx + 1])\n",
    "            else:\n",
    "                max_next_Q = discount * Q[t + 1, 0, 0]\n",
    "            td_target = reward + max_next_Q\n",
    "        else:\n",
    "            #end state\n",
    "            td_target = reward + rho * np.pow(states[next_x_idx], gamma) / gamma\n",
    "\n",
    "        # target for mean field\n",
    "        a_target = actions[a_idx]\n",
    "        \n",
    "        # Update count for learning rate\n",
    "        count_txa[t, x_idx, a_idx] += 1\n",
    "        \n",
    "        # Calculate learning rates\n",
    "        rhos = rhosCalc(count_txa[t, x_idx, a_idx], k)\n",
    "        rho_Q = rhos['q']\n",
    "        rho_Mu = rhos['mu']\n",
    "        \n",
    "        # Q-learning update\n",
    "        Q[t, x_idx, a_idx] = Q[t, x_idx, a_idx] + rho_Q * (td_target - Q[t, x_idx, a_idx])\n",
    "        \n",
    "        # Mean field distribution update\n",
    "        mu[t] = mu[t] + rho_Mu * (a_target - mu[t])\n",
    "\n",
    "        # Move to next state\n",
    "        x_idx = next_x_idx\n",
    "\n",
    "        if (k + 1) % jump == 0:\n",
    "            mu_k[t].append(mu[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualify = 200\n",
    "optimIND = []\n",
    "optim = []\n",
    "for t in range (T):\n",
    "    optim.append([])\n",
    "    optimIND.append([])\n",
    "    for x in range (size):\n",
    "        if (np.sum(count_txa[t, x]) > qualify):\n",
    "            optim[t].append(softMaxMean(Q[t, x, :], x))\n",
    "            optimIND[t].append(x)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(optimIND[0], optim[0], 'k')\n",
    "plt.plot(optimIND[1], optim[1], 'c')\n",
    "plt.plot(optimIND[2], optim[2], 'r')\n",
    "plt.plot(optimIND[3], optim[3], 'b')\n",
    "plt.legend([\"t = 0\", \"t = 1\", \"t = 2\", \"t = 3\"])\n",
    "plt.title(\"Optimal Control for Each Time and State\")\n",
    "plt.ylim([0, 4])\n",
    "plt.xlim([0, 120])\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "axs[0, 0].plot(optimIND[0], optim[0], color='blue')\n",
    "axs[0, 0].set_title('optimal policy for t = 0')\n",
    "\n",
    "axs[0, 1].plot(optimIND[1], optim[1], color='green')\n",
    "axs[0, 1].set_title('optimal policy for t = 1')\n",
    "\n",
    "axs[1, 0].plot(optimIND[2], optim[2], color='orange')\n",
    "axs[1, 0].set_title('optimal policy for t = 2')\n",
    "\n",
    "axs[1, 1].plot(optimIND[3], optim[3], color='red')\n",
    "axs[1, 1].set_title('optimal policy for t = 3')\n",
    "\n",
    "# calculating ratio of state to optimal action\n",
    "# should be a horizontal line function\n",
    "prop = []\n",
    "for t in range(T):\n",
    "    prop.append([])\n",
    "    for i in range(2, len(optimIND[t])):\n",
    "        prop[t].append(optim[t][i] / states[optimIND[t][i]])\n",
    "\n",
    "plt.plot(optimIND[0][2:], prop[0])\n",
    "plt.plot(optimIND[1][2:], prop[1])\n",
    "plt.plot(optimIND[2][2:], prop[2])\n",
    "plt.plot(optimIND[3][2:], prop[3])\n",
    "plt.legend([\"t = 0\", \"t = 1\", \"t = 2\", \"t = 3\"])\n",
    "plt.title(\"Optimal Proportion of Wealth invested at Time t / State x\")\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# trend of mean field wealth invested through episodes\n",
    "plt.plot(list(range(jump, num_episodes + jump, jump)), mu_k[0])\n",
    "plt.plot(list(range(jump, num_episodes + jump, jump)), mu_k[1])\n",
    "plt.plot(list(range(jump, num_episodes + jump, jump)), mu_k[2])\n",
    "plt.plot(list(range(jump, num_episodes + jump, jump)), mu_k[3])\n",
    "plt.legend([\"t = 0\", \"t = 1\", \"t = 2\", \"t = 3\"])\n",
    "plt.title(\"Mean Field Action throughout Q-learning episodes\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
